# Optimize_Transformer_Moder_Experiment
Experiment difference optimization method
![image](https://github.com/Nghiauet/Optimize_Transformer_Moder_Experiment/assets/63385521/dea72c11-c0fc-414e-8189-a84264e3a6e8)
The model experiment is the intents predictions use BERT base model fine tune tin CLINC150 dataset ( have 150 intents labels in 10 domains)
Techniques to speed up the predictions and reduce the memory footprint include
- Quantization
- knowledge distillation
- quantization training
- pruning
- graph optimization ( with ONNX and ORT)